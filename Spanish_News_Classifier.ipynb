{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SPANISH NEWS CLASSIFIER\n",
        "## WIP NEED LARGER SAMPLE"
      ],
      "metadata": {
        "id": "lhhopC5SmPyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJrgZe-VChdi",
        "outputId": "82542745-e375-4b6a-abf3-33895fd5e063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 77.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 36.2 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.5.1-py3-none-any.whl (10 kB)\n",
            "Collecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.12.0)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.0.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.7.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.1.1-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.7.0)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2021.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=9fe0c1f9a288ee0d77cdac6a5c67403a5bb9b1659e37fc279b1da4932912930e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.0-cp37-cp37m-linux_x86_64.whl size=99971 sha256=fded56e3ad9d2a2a99cec3fdb28001584bcc122cdab4c959593a5289488e534c\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/d4/df/08cd6e1fa4a8691b268ab254bd0fa589827ab5b65638c010b4\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=13a032038dd9ae5cc76dc3e0ec7e08fa0fa2ebaa7fed05b8c49992902a767624\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=23a3d6d21d1acb358ec8c93029fce22ae9600bd3fd2b732f092c09fc387451ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed backports.csv-1.0.7 cheroot-8.6.0 cherrypy-18.6.1 cryptography-36.0.2 feedparser-6.0.8 jaraco.classes-3.2.1 jaraco.collections-3.5.1 jaraco.context-4.1.1 jaraco.functools-3.5.0 jaraco.text-3.7.0 mysqlclient-2.1.0 pattern-3.6 pdfminer.six-20220319 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.1 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4zpvLumCLba",
        "outputId": "806916e8-a9a1-40c2-d949-b9e5f269fd5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Libraries for text preprocessing\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer#nltk.download('wordnet') \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import os\n",
        "\n",
        "### Libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "englishStemm=SnowballStemmer(\"english\").stem\n",
        "spanishStemm=SnowballStemmer(\"spanish\").stem\n",
        "import pattern\n",
        "from pattern.es import lemma as spanishlemma\n",
        "from pattern.en import lemma as englishlemma\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stpwrd = stopwords.words('spanish')\n",
        "stpwrd.extend(\"toggle\") # THE TRAINING DATA IS LITTERED WITH THIS, SO I USE THIS STOPWORD TO REMOVE IT\n",
        "stopEnglish=set(stopwords.words('english'))\n",
        "stopSpanish=set(stpwrd)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV,train_test_split\n",
        "import time\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/Portfolios/data_larazon_publico_v2.csv', delimiter=',', header=0)\n",
        "df = df.fillna(0)\n",
        "df = df.drop(columns=['Unnamed: 0', 'indi'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4YWAc4mCaQu",
        "outputId": "3fac2f63-5a10-410c-cf88-5c67b54e3513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nL4caTHCLbh"
      },
      "outputs": [],
      "source": [
        "class preprocesaTexto():\n",
        "    def __init__(self, idioma,_tokeniza=False,_aMinusculas=True,\n",
        "                 _aMayusculas=False,_removerPuntuacion=True,_removerCaracteresRaros=True,\n",
        "                 _quitarNumeros=True,_lematiza=False,_stemming=False,_quitarAcentos=False,\n",
        "                 _remueveStop=False,_muestraCambios=False):\n",
        "      self.idioma=idioma\n",
        "      self._tokeniza=_tokeniza\n",
        "      self._aMinusculas=_aMinusculas\n",
        "      self._aMayusculas=_aMayusculas\n",
        "      self._removerPuntuacion=_removerPuntuacion\n",
        "      self._removerCaracteresRaros=_removerCaracteresRaros\n",
        "      self._quitarNumeros=_quitarNumeros\n",
        "      self._lematiza=_lematiza\n",
        "      self._stemming=_stemming\n",
        "      self._quitarAcentos =_quitarAcentos\n",
        "      self._remueveStop =_remueveStop\n",
        "      self._muestraCambios=_muestraCambios\n",
        "        \n",
        "    def tokeniza(self,texto):\n",
        "      return word_tokenize(texto)\n",
        "\n",
        "    def aMinusculas(self,texto):\n",
        "      if self.idioma== 'es':\n",
        "        ans=\"\"\n",
        "        for c in texto:\n",
        "          if c=='Á' : c='á'\n",
        "          if c=='É' : c='é'\n",
        "          if c=='Í' : c='í'\n",
        "          if c=='Ó' : c='ó'\n",
        "          if c=='Ú' : c='ú'\n",
        "          if c=='Ñ' : c='ñ'\n",
        "          if c=='Ü' : c='ü'\n",
        "          ans+=c\n",
        "        return ans.lower()\n",
        "      else :\n",
        "        return texto.lower()\n",
        "\n",
        "    def aMayusculas(self,texto):\n",
        "      if self.idioma == 'es':\n",
        "        ans=\"\"\n",
        "        for c in texto:\n",
        "          if c=='á' : c='Á'\n",
        "          if c=='é' : c='É'\n",
        "          if c=='í' : c='Í'\n",
        "          if c=='ó' : c='Ó'\n",
        "          if c=='ú' : c='Ú'\n",
        "          if c=='ñ' : c='Ñ'\n",
        "          if c=='ü' : c='Ü'\n",
        "          ans+=c\n",
        "        return ans.upper()\n",
        "      else :\n",
        "        return texto.upper()\n",
        "\n",
        "    def quitarAcentos(self,texto):\n",
        "      if self.idioma == 'es':\n",
        "        ans=\"\"\n",
        "        for c in texto:\n",
        "          if c=='á' : c='a'\n",
        "          if c=='é' : c='e'\n",
        "          if c=='í' : c='i'\n",
        "          if c=='ó' : c='o'\n",
        "          if c=='ú' : c='u'\n",
        "          if c=='Á' : c='A'\n",
        "          if c=='É' : c='E'\n",
        "          if c=='Í' : c='I'\n",
        "          if c=='Ó' : c='O'\n",
        "          if c=='Ú' : c='U'\n",
        "          ans+=c\n",
        "        return ans\n",
        "      else :\n",
        "        return texto\n",
        "\n",
        "    def removerPuntuacion(self,texto):\n",
        "      puntuacion=string.punctuation\n",
        "      if self.idioma=='es': \n",
        "        puntuacion+=\"¡¿\"\n",
        "      temp=\"\"\n",
        "      for c in texto:\n",
        "        if not c in puntuacion:\n",
        "          temp+=c\n",
        "      return temp\n",
        "\n",
        "    def removerCaracteresRaros(self,texto):\n",
        "      buenos=string.printable\n",
        "      if self.idioma=='es': \n",
        "        buenos+=\"¡¿áéíóúÁÉÍÓÚÑñÜü\"\n",
        "      temp=\"\"\n",
        "      for c in texto:\n",
        "        if c in buenos:\n",
        "          temp+=c\n",
        "      return temp\n",
        "\n",
        "    def quitarNumeros(self,texto):\n",
        "      temp=\"\"\n",
        "      for c in texto:\n",
        "        if not c.isdigit():\n",
        "          temp+=c\n",
        "      return temp\n",
        "\n",
        "    def lematiza(self,texto):\n",
        "      palabras=word_tokenize(texto)\n",
        "      temp=[]\n",
        "      if self.idioma== 'es':\n",
        "        for pal in palabras:\n",
        "          temp.append(spanishlemma(pal))\n",
        "          temp.append(\" \")\n",
        "      else :\n",
        "        for pal in palabras:\n",
        "          temp.append(englishlemma(pal))\n",
        "          temp.append(\" \")\n",
        "      return \"\".join(temp)\n",
        "\n",
        "    def stemming(self,texto):\n",
        "      palabras=word_tokenize(texto)\n",
        "      temp=[]\n",
        "      if self.idioma== 'es':\n",
        "        for pal in palabras:\n",
        "          temp.append(spanishStemm(pal))\n",
        "          temp.append(\" \")\n",
        "      else :\n",
        "        for pal in palabras:\n",
        "          temp.append(englishStemm(pal))\n",
        "          temp.append(\" \")\n",
        "      return \"\".join(temp)\n",
        "\n",
        "    def remueveStop(self,texto):\n",
        "      palabras=word_tokenize(texto)\n",
        "      temp=[]\n",
        "      if self.idioma== 'es':\n",
        "        for pal in palabras:\n",
        "          if pal not in stopSpanish:\n",
        "            temp.append(pal)\n",
        "            temp.append(\" \")\n",
        "      else :\n",
        "        for pal in palabras:\n",
        "          if pal not in stopEnglish:\n",
        "            temp.append(pal)\n",
        "            temp.append(\" \")\n",
        "      return \"\".join(temp)\n",
        "\n",
        "    def preprocesa(self,texto):\n",
        "      ans=texto\n",
        "      if self._muestraCambios : print(\"Inicial:\\n\", ans)\n",
        "      if self._removerCaracteresRaros : \n",
        "        ans=self.removerCaracteresRaros(ans)\n",
        "        if self._muestraCambios : print(\"Quitando caracteres raros:\\n\", ans)\n",
        "      if self._removerPuntuacion : \n",
        "        ans=self.removerPuntuacion(ans)\n",
        "        if self._muestraCambios :print(\"Quitando signos de puntuacion:\\n\", ans)\n",
        "      if self._quitarNumeros : \n",
        "        ans=self.quitarNumeros(ans)\n",
        "        if self._muestraCambios : print(\"Quitando numeros:\\n\", ans)\n",
        "      if self._aMinusculas : \n",
        "        ans=self.aMinusculas(ans)\n",
        "        if self._muestraCambios : print(\"Convirtiendo a minusculas:\\n\", ans)  \n",
        "      if self._aMayusculas : \n",
        "        ans=self.aMayusculas(ans)\n",
        "        if self._muestraCambios : print(\"Convirtiendo a mayusculas:\\n\", ans)\n",
        "      if self._remueveStop : \n",
        "        ans=self.remueveStop(ans)\n",
        "        if self._muestraCambios : print(\"Quitando stop words:\\n\", ans)\n",
        "      if self._quitarAcentos : \n",
        "        ans=self.quitarAcentos(ans)\n",
        "        if self._muestraCambios : print(\"Quitando acentos:\\n\", ans)\n",
        "      if self._lematiza : \n",
        "        ans=self.lematiza(ans)\n",
        "        if self._muestraCambios : print(\"Lematizando:\\n\", ans)\n",
        "      if self._stemming : \n",
        "        ans=self.stemming(ans)\n",
        "        if self._muestraCambios : print(\"Stemming:\\n\", ans)\n",
        "      if self._tokeniza : \n",
        "        ans=self.tokeniza(ans)\n",
        "        if self._muestraCambios : print(\"Tokenizando:\\n\", ans)\n",
        "      if self._muestraCambios : print(\"Final:\\n\", ans)\n",
        "      return ans \n",
        "\n",
        "preprocesador = preprocesaTexto(idioma='es',_tokeniza=False,\n",
        "                                _muestraCambios=False,_quitarAcentos=True,\n",
        "                                _remueveStop=True,_stemming=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEWS CLASSIFICATION SUPERVISED"
      ],
      "metadata": {
        "id": "NYzNiYqWDu_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df.iloc[:,1]\n",
        "y_train = df.iloc[:,0]"
      ],
      "metadata": {
        "id": "HZx-0RLjC9I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcabGxuuCLbq",
        "outputId": "e1e7f73c-5112-4121-beee-c2e069492b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Read training data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "TrainingData = pd.read_csv('/content/drive/MyDrive/Portfolios/Training_data.csv')\n",
        "X_train = TrainingData.iloc[:,1]\n",
        "y_train = TrainingData.iloc[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAr0QAjnCLbr",
        "outputId": "7ed5e90c-42bf-4d48-96e1-28465d5f3878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing grid search...\n",
            "parameters:\n",
            "{'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': ((1, 1), (1, 2)), 'clf__max_iter': (10,), 'clf__alpha': (1e-05, 1e-06), 'clf__penalty': ('l2', 'elasticnet')}\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        }
      ],
      "source": [
        "# #############################################################################\n",
        "# Define a pipeline combining a text feature extractor with a simple\n",
        "# classifier\n",
        "pipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=39, max_iter=5, tol=None)), ])\n",
        "\n",
        "\n",
        "# find the best parameters for both the feature extraction and the\n",
        "# classifier\n",
        "# uncommenting more parameters will give better exploring power but will\n",
        "# increase processing time in a combinatorial way\n",
        "parameters = {\n",
        "    \"vect__max_df\": (0.5, 0.75, 1.0),\n",
        "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
        "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
        "    # 'tfidf__use_idf': (True, False),\n",
        "    # 'tfidf__norm': ('l1', 'l2'),\n",
        "    \"clf__max_iter\": (10,),\n",
        "    \"clf__alpha\": (0.00001, 0.000001),\n",
        "    \"clf__penalty\": (\"l2\", \"elasticnet\"),\n",
        "    # 'clf__max_iter': (10, 50, 80),\n",
        "}\n",
        "\n",
        "# find the best parameters for both the feature extraction and the\n",
        "# classifier\n",
        "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
        "\n",
        "print(\"Performing grid search...\")\n",
        "print(\"parameters:\")\n",
        "print(parameters)\n",
        "start_time = time.time()\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "    print((time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho9N4g6_CLbt"
      },
      "outputs": [],
      "source": [
        "# Best parameter estimator\n",
        "pipeline2 = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), \n",
        "                      ('clf', SGDClassifier(loss='hinge', penalty='elasticnet',alpha=1e-5, random_state=39, max_iter=10, tol=None)), ])\n",
        "\n",
        "# Fit the best SGDClassifier to train data\n",
        "pipeline2.fit(X_train, y_train)\n",
        "\n",
        "import joblib\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "joblib.dump(pipeline2, filename)\n",
        "\n",
        "# load the model from disk\n",
        "loaded_model = joblib.load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtXGQuO3CLbv"
      },
      "outputs": [],
      "source": [
        "# Predict topics SGDClassifier\n",
        "data= pd.read_csv(\"data/Ejercicio_notas.csv\")\n",
        "corpus2 = []\n",
        "for i in range(0, len(data['texto'])):\n",
        "    corpus2.append(preprocesador.preprocesa(data['texto'][i]))\n",
        "# Fit the best SGDClassifier to train data\n",
        "loaded_model.predict(corpus2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Spanish News Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}